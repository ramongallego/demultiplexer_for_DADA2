---
title: "Dada2_report"
author: "Belen_rawdata_trim270_210"
date: ""
output: html_document
params:
  folder:
    value: /home/mk1b/Projects/nsDNA/pipeline_output/demultiplexed_20221123_1513
  hash:
    value: "yes"
  cont:
    value: z
  fastqs:
    value:  /home/mk1b/Projects/nsDNA/pipeline_output/demultiplexed_20221123_1513/demultiplexed

  original:
    value: b

---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir=params$folder)
```

## Dada2 report

You have successfully split your libraries into a pair (or two) of fastq files per sample. Now let's import the output of demultiplex_both_fastq.sh

First load the packages. And let's update the parameters we need
```{r loading packages, echo=FALSE ,message=FALSE}
#library (devtools)
library (tidyverse)
library (stringr)
library (dada2)
library (Biostrings)
library (digest)
library (rlang)

#fastq.folder="/Users/rgallego/fastqs_demultiplexed_for_DADA2/demultiplexed_20180108_1539"
sample.map <- read_delim(paste0(params$folder,"/sample_trans.tmp"),col_names = c("Full_Id", "fastq_header","Sample"),delim = "\t")
head (sample.map)

path1 <- params$fastqs
params$original[1]
Sys.setenv(READ1=params$original[1])
Sys.setenv(READ2=params$original[2])

# Function to summarise data

getN <- function(x) sum(getUniques(x))

```
Firstly, we'll find the patterns that separate our Fwd, Rev, .1 and .2 files. look at the quality of the .1 and .2 reads
```{r listing files}
version$version.string

F1s <- sort(list.files(path1, pattern="_Fwd.1.fastq", full.names = TRUE))
F2s <- sort(list.files(path1, pattern="_Fwd.2.fastq", full.names = TRUE))
R1s <- sort(list.files(path1, pattern="_Rev.1.fastq", full.names = TRUE))
R2s <- sort(list.files(path1, pattern="_Rev.2.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- str_replace(basename(F1s), "_Fwd.1.fastq","")

#Now use only those that reflect a real sample

good.sample.names<-sample.names[sample.names %in% sample.map$fastq_header]
# Introduce here the biological counterpart of the fastq file

real.sample.name <- sample.map[,3][match(good.sample.names,sample.map$fastq_header),1]

# Now subset the number of files to be looked at

F1sgood<-F1s[sample.names %in% sample.map$fastq_header]
F2sgood<-F2s[sample.names %in% sample.map$fastq_header]
R1sgood<-R1s[sample.names %in% sample.map$fastq_header]
R2sgood<-R2s[sample.names %in% sample.map$fastq_header]

```



## Now start with the trimming of each read based on the quality we just plotted
```{r filter and trim}
filt_path <- file.path(params$folder, "/filtered_220_120") # Place filtered files in filtered/ subdirectory
filtF1s <- file.path(filt_path, paste0(good.sample.names, "_F1_filt.fastq.gz"))
filtF2s <- file.path(filt_path, paste0(good.sample.names, "_F2_filt.fastq.gz"))
out_Fs <- filterAndTrim(F1sgood, filtF1s, F2sgood, filtF2s,
   truncLen=c(220,120),
                      maxN=0, maxEE=c(2,2),
   # truncQ=10, rm.phix=TRUE,
                      compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE

filtR1s <- file.path(filt_path, paste0(good.sample.names, "_R1_filt.fastq.gz"))
filtR2s <- file.path(filt_path, paste0(good.sample.names, "_R2_filt.fastq.gz"))
out_Rs <- filterAndTrim(R1sgood, filtR1s, R2sgood, filtR2s,
  truncLen=c(220,120),
                      maxN=0, maxEE=c(2,2),
  # truncQ=10, rm.phix=TRUE,
                      compress=TRUE, multithread=TRUE)
```
A nice idea is to plot the Quality of the F1 reads

```{r qplot1, echo=FALSE}
plotQualityProfile(filtF1s[1])
```

The quality should be similar to that of the Reverse .1 reads

```{r plot2, echo=FALSE}
plotQualityProfile(filtR1s[1])
```

On the other hand, the quality of the .2 reads should decrease earlier
```{r qplot3, echo=FALSE}
plotQualityProfile(filtF2s[2])
```

```{r as tibble}
out_Fs_tibble<-tibble(file=dimnames(out_Fs)[[1]], reads.in = out_Fs[,1], reads.out=out_Fs[,2], direction = "Fwd")
out_Rs_tibble<-tibble(file=dimnames(out_Rs)[[1]], reads.in = out_Rs[,1], reads.out=out_Rs[,2], direction = "Rev")

bind_rows(out_Fs_tibble, out_Rs_tibble) %>%
  group_by(direction) %>%
  summarise (across(where(is.numeric), sum))

```
Before calculating errors, it is best to remove samples that have 0 or few reads.

```{r removing samples 1}
out_Fs_tibble %>%
  filter (reads.out > 20) %>%
  mutate (key = str_remove(file, "_Fwd.1.fastq")) %>%
  select(key)-> filt_Fs_good

out_Rs_tibble %>%
  filter (reads.out > 20) %>%
  mutate (key = str_remove(file, "_Rev.1.fastq")) %>%
  select(key)-> filt_Rs_good

## Select only those files in the lists above

tibble(Filenames = filtF1s) %>%
  separate(Filenames, into = c(NA, "key"), sep = "/filtered_220_120/", remove = F ) %>%
  mutate (key = str_remove(key, "_F1_filt.fastq.gz")) %>%
  inner_join(filt_Fs_good) %>%
  pull(Filenames) -> filtF1s_good

tibble(Filenames = filtF2s) %>%
  separate(Filenames, into = c(NA, "key"), sep = "/filtered_220_120/", remove = F ) %>%
  mutate (key = str_remove(key, "_F2_filt.fastq.gz")) %>%
  inner_join(filt_Fs_good) %>%
  pull(Filenames) -> filtF2s_good

### Same for Rs
tibble(Filenames = filtR1s) %>%
  separate(Filenames, into = c(NA, "key"), sep = "/filtered_220_120/", remove = F ) %>%
  mutate (key = str_remove(key, "_R1_filt.fastq.gz")) %>%
  inner_join(filt_Rs_good) %>%
  pull(Filenames) -> filtR1s_good

tibble(Filenames = filtR2s) %>%
  separate(Filenames, into = c(NA, "key"), sep = "/filtered_220_120/", remove = F ) %>%
  mutate (key = str_remove(key, "_R2_filt.fastq.gz")) %>%
  inner_join(filt_Rs_good) %>%
  pull(Filenames) -> filtR2s_good

```


Now the first crucial step: learning the error rates that would be different for fwd and rev reads

```{r learning errors, echo=T}
errF1 <- learnErrors(filtF1s_good, multithread=TRUE,verbose = 0)
errF2 <- learnErrors(filtF2s_good, multithread=TRUE,verbose = 0)
errR1 <- learnErrors(filtR1s_good, multithread=TRUE,verbose = 0)
errR2 <- learnErrors(filtR2s_good, multithread=TRUE,verbose = 0)

# Write errors to csv to see if they matter at all
tosave <- list(errF1, errF2, errR1, errR2)

saveRDS(tosave, file = "all.errors.rds")
rm(tosave)
gc()
#
# loaded <- read_rds(file.path(params$folder,"all.errors.rds"))
# errF1 <- loaded[[1]]
# errF2 <- loaded[[2]]
# errR1 <- loaded[[3]]
# errR2 <- loaded[[4]]
# rm(loaded)
```

Which we can plot now to see the error rates between transitions of each pair of nt
```{r plotErrors}

plotErrors(errF1, nominalQ = T)
plotErrors(errF2, nominalQ = T)
plotErrors(errR1, nominalQ = T)
plotErrors(errR2, nominalQ = T)


```

## Now go to the dereplication step

```{r dereplication, echo=F,message=FALSE}

# Name the derep-class objects by the sample names

rownames(out_Fs) <- real.sample.name$Sample
sample_map<- sample.map %>%
select(key = fastq_header, Sample)

filt_Fs_good %>%
  left_join(sample_map) -> filt_Fs_good


derepF1s <- derepFastq(filtF1s_good, verbose=TRUE)

derepF2s <- derepFastq(filtF2s_good, verbose=TRUE)


names(derepF1s) <- names(derepF2s) <- filt_Fs_good$Sample
write_rds(derepF1s, "derepF1s.rds")
write_rds(derepF2s, "derepF2s.rds")
rm(derepF2s, derepF1s)
gc()
# Now the other half

filt_Rs_good %>%
  left_join(sample_map) -> filt_Rs_good

rownames(out_Rs) <- real.sample.name$Sample

derepR1s <- derepFastq(filtR1s_good, verbose=TRUE)


 names(derepR1s) <-filt_Rs_good$Sample
 write_rds(derepR1s, "derepR1s.rds")

rm(derepR1s)
gc()

derepR2s <- derepFastq(filtR2s_good, verbose=TRUE)
 names(derepR2s) <- filt_Rs_good$Sample
write_rds(derepR2s, "derepR2s.rds")
rm(derepR2s)
gc()







```

## And finally an inference of the sample composition

```{r dadaing, message=FALSE}
gc()
derepF1s <- read_rds("derepF1s.rds")
dadaF1s <- dada(derepF1s, err = errF1, multithread = TRUE)
rm (derepF1s)
gc()

derepF2s <- read_rds("derepF2s.rds")
dadaF2s <- dada(derepF2s, err = errF2, multithread = TRUE)
rm (derepF2s)
gc()

derepR1s <- read_rds("derepR1s.rds")
dadaR1s <- dada(derepR1s, err = errR1, multithread = TRUE)
rm(derepR1s)
gc()

derepR2s <- read_rds("derepR2s.rds")
dadaR2s <- dada(derepR2s, err = errR2, multithread = TRUE)
rm(derepR2s)
gc()
```

## We are ready now to merge reads - using the denoised reads and the derep files.
We will start by saving all files to disk.
```{r saving to disk}


   saveRDS(list( dadaF1s, dadaF2s, dadaR1s,dadaR2s), file = "tosave.rds")

# loaded <- read_rds("tosave.rds")
# dadaF1s <- loaded[[1]]
# dadaF2s <- loaded[[2]]
# dadaR1s <- loaded[[3]]
# dadaR2s <- loaded[[4]]
# rm(loaded)
```
The pipeline breaks if for some reason there are samples that don't have any reads passing filters
(or at least that is the current hypothesis)

Let's try to subset only those samples with more than 1000 reads on each direction

```{r merging pairs}

to.keep.F <- map_lgl(dadaF1s, ~(sum(.x$denoised) > 150)) # Which samples have more than 150 reads passing filters in the Fwd direction
to.keep.R <- map_lgl(dadaR1s, ~(sum(.x$denoised) > 150)) #  Which samples have more than 150 reads passing filters in the Rev direction

#to.keep <- (to.keep.F + to.keep.R) == 2 # Keep only those that pass boths filters
derepF1s <- read_rds("derepF1s.rds")
derepF2s <- read_rds("derepF2s.rds")
mergersF <- mergePairs(dadaF1s[to.keep.F],
   derepF1s[to.keep.F],
   dadaF2s[to.keep.F],
   derepF2s[to.keep.F],
   verbose = 0)

#Run a for loop that adds the number of unique reads that went into each ASV

for (j in 1:length(mergersF)){

  dadaF1s[[j]]@.Data[[2]] %>% rownames_to_column(var="forward") %>% select("forward", "nunq") ->Fwd
  Fwd$forward<-as.integer(Fwd$forward)
  dadaF2s[[j]]@.Data[[2]] %>% rownames_to_column(var="reverse") %>% select("reverse", "nunq") ->Rev
  Rev$reverse<-as.integer(Rev$reverse)

  mergersF[[j]] <- left_join(mergersF[[j]],Fwd, by="forward") %>% left_join(Rev, by="reverse") %>% mutate(nunq=pmin(nunq.x,nunq.y)) %>% select(-nunq.x,-nunq.y)


}

summary.fun <- function(list.of.step){

  name.step <- deparse(substitute(list.of.step))

  map_dfc(list.of.step, getN) %>%
   pivot_longer(everything(), names_to = "sample", values_to = "value") %>%
  mutate(step = name.step)
}

summary.fun(dadaF1s) %>%
  bind_rows(summary.fun(dadaF2s)) %>%
  bind_rows(summary.fun(dadaR1s)) %>%
  bind_rows(summary.fun(dadaR2s)) %>%
  bind_rows(summary.fun(derepF1s)) %>%
  bind_rows(summary.fun(derepF2s)) %>%
  bind_rows(summary.fun(mergersF))-> summary.stats


out_Fs %>%
  as.data.frame() %>%
  rownames_to_column("file") %>%
  dplyr::rename(reads.in.fwd =2, filtered.fwd = 3) %>%
  pivot_longer(-file, names_to = "step") %>%
  mutate (fastq_header = str_remove(file, "_Fwd.1.fastq")) %>%
  left_join(sample.map) %>%
  select(sample = Sample, step, value) %>%
  bind_rows(summary.stats) -> summary.stats

rm(dadaF1s, dadaF2s, derepF1s, derepF2s)

gc()

derepR1s <- read_rds("derepR1s.rds")
derepR2s <- read_rds("derepR2s.rds")
mergersR <- mergePairs(dadaR1s[to.keep.R],
   derepR1s[to.keep.R],
   dadaR2s[to.keep.R],
   derepR2s[to.keep.R],
   verbose = 0)

for (j in 1:length(mergersR)){

  dadaR1s[[j]]@.Data[[2]] %>% rownames_to_column(var="forward") %>% select("forward", "nunq") ->Fwd
  Fwd$forward<-as.integer(Fwd$forward)
  dadaR2s[[j]]@.Data[[2]] %>% rownames_to_column(var="reverse") %>% select("reverse", "nunq") ->Rev
  Rev$reverse<-as.integer(Rev$reverse)

  mergersR[[j]] <- left_join(mergersR[[j]],Fwd, by="forward") %>% left_join(Rev, by="reverse") %>% mutate(nunq=pmin(nunq.x,nunq.y)) %>% select(-nunq.x,-nunq.y)

}

summary.stats %>%
  bind_rows(summary.fun(derepR1s)) %>%
  bind_rows(summary.fun(derepR2s)) %>%
  bind_rows(summary.fun(mergersR)) -> summary.stats

out_Rs %>%
  as.data.frame() %>%
  rownames_to_column("file") %>%
  dplyr::rename(reads.in.rev =2, filtered.rev = 3) %>%
  pivot_longer(-file, names_to = "step") %>%
  mutate (fastq_header = str_remove(file, "_Rev.1.fastq")) %>%
  left_join(sample.map) %>%
  select(sample = Sample, step, value) %>%
  bind_rows(summary.stats) -> summary.stats
rm(derepR1s, derepR2s, dadaR1s, dadaR2s)
gc()
```

## Now we have to merge the Forward and Reverse Reads to make a unique object

Step 1 is to create sequence tables for each direction, seqtabF and seqtabR
```{r merging F and R (1)}

seqtabF <- makeSequenceTable(mergersF)

dim(seqtabF)

rowSums(seqtabF) %>%
  as.data.frame() %>%
  rownames_to_column("sample") %>%
  dplyr::rename(value = 2) %>%
  mutate(step = "Tabled.fwd") %>%
  bind_rows(summary.stats) -> summary.stats

table(nchar(getSequences(seqtabF)))

seqtabR <- makeSequenceTable(mergersR)

dim(seqtabR)

nchar(getSequences(seqtabR)) %>% as.data.frame() %>%
  dplyr::rename(length = 1) %>%
  ggplot(aes(x = length)) + geom_density()

rowSums(seqtabR) %>%
  as.data.frame() %>%
  rownames_to_column("sample") %>%
  dplyr::rename(value = 2) %>%
  mutate(step = "Tabled.rev") %>%
  bind_rows(summary.stats) -> summary.stats
```


Step  2 is to reverse complement the reverse reads, and checking how many of the reads in F were present in R (if you get a 0 means we have done something wrong). Lastly, we change the reverse sequences and add their Reverse complement instead
```{r merging F and R (2)}
reversed_sequences<-as.character(reverseComplement(DNAStringSet(colnames(seqtabR))))

summary (colnames(seqtabF) %in% reversed_sequences)

summary (reversed_sequences %in% colnames(seqtabF))

colnames(seqtabR)<-reversed_sequences

```

Step 3 does the actual merging and returns another sequence Table

```{r merging F and R (3)}

seqtab.R.df=as.data.frame(seqtabR)

seqtab.R.df %>%
  rownames_to_column("sample") %>%
  pivot_longer(-sample, names_to = "sequence", values_to = "nReads_R") -> seqtab.R.df

seqtab.F.df=as.data.frame(seqtabF)
seqtab.F.df %>%
  rownames_to_column("sample") %>%
  pivot_longer(-sample, names_to = "sequence", values_to = "nReads_F") -> seqtab.F.df

full_join(seqtab.R.df, seqtab.F.df) %>%
  replace_na(list(nReads_R = 0, nReads_F = 0)) %>%
  group_by(sample,sequence ) %>%
  mutate(nReads = sum(nReads_R + nReads_F)) %>%
  select(sample, sequence, nReads)  %>%
  pivot_wider(id_cols = sample, names_from = sequence, values_from = nReads) -> full_df



final.seqtab <- full_df %>%
  ungroup() %>%
  select(-sample) %>%
  as.matrix()

dimnames(final.seqtab)[[1]] <- full_df$sample


rowSums(final.seqtab) %>%
  as.data.frame() %>%
  rownames_to_column("sample") %>%
  dplyr::rename(value = 2) %>%
  mutate(step = "Tabled") %>%
  bind_rows(summary.stats) -> summary.stats


```

## Now get rid of the chimeras

```{r RemovingChimeras, message=F}

seqtab.nochim <- removeBimeraDenovo(final.seqtab, method="consensus", multithread=TRUE)

dim(seqtab.nochim)

table(nchar(getSequences(seqtab.nochim)))

```



## IF selected, proceed with Hashing: create a hash conversion table and saving files in tidyr format

We are going to keep the info in a tidyr format and save it into a csv file
```{r tidying and writing}

seqtab.nochim.df=as.data.frame(seqtab.nochim)

ASV.file <- file.path(params$folder, "ASV_table_220.csv")

# Now decide if you want hashing or not

if (grepl ("yes", params$hash, ignore.case = TRUE)) {

  conv_file <-  file.path(params$folder,"hash_key_220.csv")

  conv_table <- tibble( Hash = "", Sequence ="")

  #hashes <- list(NULL)

  map_chr (colnames(seqtab.nochim.df), ~ digest(.x, algo = "sha1", serialize = F, skip = "auto")) -> Hashes
  conv_table <- tibble (Hash = Hashes,
                        Sequence = colnames(seqtab.nochim.df))

  colnames(seqtab.nochim.df) <- Hashes


  write_csv(conv_table, conv_file)


seqtab.nochim.df %>%
  rownames_to_column("sample") %>%
  pivot_longer(-sample, names_to="Hash", values_to = "nReads") %>%
  filter (nReads > 0) -> current_asv

write_csv(current_asv, ASV.file)

} else { #What do we do if you don't want hashes: two things - Change the header of the ASV table, write only one file

  seqtab.nochim.df %>%
  rownames_to_column("sample") %>%
  pivot_longer(-sample, names_to="Sequence", values_to = "nReads") %>%
  filter (nReads > 0) ->  current_asv
  write_csv(current_asv, ASV.file)
}


```



```{r}
current_asv %>%
  group_by(sample) %>%
  summarise(value = sum(nReads)) %>%
  mutate(step = "ASV") %>% bind_rows(summary.stats) -> summary.stats


summary.stats %>%
  mutate(direction = case_when(str_detect(step, "fwd|F")~"Fwd",
                               str_detect(step, "rev|R")~"Rev",
                               TRUE                     ~ "Both") ) -> summary.stats


summary.stats %>%
  mutate(file = case_when(str_detect(step, "1s")~ ".1",
                               str_detect(step, "2s")~ ".2",
                               TRUE ~ "Both")) %>%
  filter (!str_detect(step,"derep")) %>%
  mutate(Step = fct_recode(step, Reads.in = "reads.in.rev",
                           Reads.in = "reads.in.fwd",
                           Filtered = "filtered.rev",
                           Filtered = "filtered.fwd",
                           Denoised = "dadaF1s",
                           Denoised = "dadaF2s",
                           Denoised = "dadaR1s",
                           Denoised = "dadaR2s",
                           Merged   = "mergersF",
                           Merged   = "mergersR",
                           Tabled   = "Tabled.fwd",
                           Tabled   = "Tabled.rev",
                           FUSION   = "Tabled",
                           ASV      = "ASV"),
         Step = fct_relevel(Step, "Reads.in", "Filtered", "Denoised", "Merged", "Tabled", "FUSION" )) ->summary.stats

current_asv %>%
  group_by(sample) %>%
  summarise(value = sum(nReads)) %>%
  mutate(step = "ASV")


write_csv(summary.stats, "dada2_summary_220.csv")

summary.stats %>%
  filter(file !=".1") %>%
  ggplot(aes(x = Step, y = value, fill = direction)) +
  geom_col(position = "stack") +
  scale_y_continuous(n.breaks = 15)
```
